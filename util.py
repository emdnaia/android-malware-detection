import glob
import json
import csv
import os
import numpy as np
from datetime import datetime
from collections import Counter
import hashlib
import matplotlib.pyplot as plt

def create_parent_folder(file_path):
    parent_folder = os.path.dirname(file_path)
    os.makedirs(parent_folder, exist_ok=True)


def list_type_files(folder_path, type):
    apk_files = glob.glob(folder_path + '/**/*.{0}'.format(type), recursive=True)
    return apk_files


def read_from_file(filePath):
    with open(filePath, 'r') as f:
        return json.load(f)

def find_duplicates(array):
    seen = set()
    duplicates = set()

    for item in array:
        if item in seen:
            duplicates.add(item)
        else:
            seen.add(item)

    return duplicates

def save_key_value_dict_to_csv(dict, headers, save_file):
    # Save the percentages to a CSV file
    with open(save_file, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(headers)  # Write the header row
        for item, percentage in dict.items():
            writer.writerow([item, percentage])

def save_key_value_dict_to_csv(dict, headers, save_file):
    # Save the percentages to a CSV file
    with open(save_file, "w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(headers)  # Write the header row
        for item, percentage in dict.items():
            writer.writerow([item, percentage])

def load_csv_to_key_value(csv_file):
    data = {}
    with open(csv_file, 'r') as csv_file:
        reader = csv.reader(csv_file)
        next(reader)  # Skip the header row
        for row in reader:
            data[row[0]] = row[1]
    return data

def remove_files(folder_path):
    # Iterate over the files in the folder
    for root, dirs, files in os.walk(folder_path):
        for file_name in files:
            file_path = os.path.join(root, file_name)
            os.remove(file_path)

def accurracy(preds, expected_value):
    count = np.count_nonzero(preds == expected_value)
    return count/preds.shape[0]

def concatnp_vertical(array1, array2):
    result = np.vstack((array1, array2))
    return result

def concatnp_horizontal(array1, array2):
    result = np.hstack((array1, array2))
    return result

def shuffle(x_train, y_train):
        # Get the number of samples
    num_samples = x_train.shape[0]
    # Create a random permutation of indices
    indices = np.random.permutation(num_samples)
    # Shuffle `x_train` and `y_train` using the indices
    shuffled_x_train = x_train[indices]
    shuffled_y_train = y_train[indices]
    return shuffled_x_train, shuffled_y_train

def split_matrix_by_rate(matrix, rate):
    split = int(matrix.shape[0]*rate)
    return matrix[:split, :], matrix[split:, :]

def save_json_file(data, save_file):
    with open(save_file, 'w') as f:
        f.write(json.dumps(data))

def get_humman_readable_time():
    current_time = datetime.now()
    # Format the time in a human-readable way
    human_readable_time = current_time.strftime('%Y-%m-%d %H:%M:%S')
    return human_readable_time

def save_counter_to_csv(counter_data, csv_file):
    # Sort the Counter items in descending order
    sorted_items = counter_data.most_common()

    # Save the sorted Counter data to a CSV file
    with open(csv_file, 'w', newline='') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Item', 'Count'])  # Header row
        for item, count in sorted_items:
            writer.writerow([item, count])

def remove_all_zero_data(npdata):
    rows_with_all_zeros = np.all(np.array(npdata) == 0, axis=1)
    rows_to_remove = np.where(rows_with_all_zeros)[0]
    mask = np.ones(npdata.shape[0], dtype=bool)
    mask[rows_to_remove] = False
    filtered_data = npdata[mask]
    return filtered_data

def calculate_sha256(file_path):
    sha256_hash = hashlib.sha256()

    with open(file_path, "rb") as f:
        # Read the file in small chunks to handle large files efficiently
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()

def find_common_items(list_of_lists):
    if not list_of_lists:
        return []

    common_items = set(list_of_lists[0])
    for sublist in list_of_lists[1:]:
        common_items = common_items.intersection(sublist)

    return list(common_items)

def save_train_history(history, save_path):
    plt.plot(history['accuracy'], label='Training Accuracy')
    plt.plot(history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.title('Training and Validation Accuracy')
    plt.savefig(save_path)

def calculate_length_between_characters(input_string, char1, char2):
    index1 = input_string.index(char1)
    index2 = input_string.index(char2)
    length = abs(index2 - index1) - 1
    return length

if __name__ == '__main__':
    #print(find_common_items([[0,1],[1,0]]))
    # print(calculate_sha256('/home/debian/apk_path/mb/apks/5f8a1f85224029228477c3f32f4d7700c511f96d1604a1d64f6f14d79a160830.apk'))
    #print(calculate_length_between_characters("Lai$a", "L", "c"))
    pass
  

